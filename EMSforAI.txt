æ—¢ç„¶æ”¹ä¸º**â€œäººå·¥å½•å…¥ï¼ˆç‚¹æ£€/ç»´ä¿æ•°æ®ï¼‰ + AI ç¦»çº¿åˆ†æâ€**çš„æ¨¡å¼ï¼ŒæŠ€æœ¯å®ç°çš„é‡ç‚¹å°±ä»â€œé«˜å¹¶å‘æ—¶åºå¤„ç†â€è½¬å‘äº†**â€œçµæ´»çš„æ•°æ®æ¨¡å‹è®¾è®¡â€**å’Œ**â€œå°æ ·æœ¬/ç¨€ç–æ•°æ®çš„è¶‹åŠ¿æ‹Ÿåˆâ€**ã€‚

è¿™ç§åœºæ™¯ä¸‹ï¼ŒAI çš„æ ¸å¿ƒä½œç”¨ä¸å†æ˜¯æ¯«ç§’çº§æŠ¥è­¦ï¼Œè€Œæ˜¯ï¼š**å¡«è¡¥æ•°æ®ç©ºç¼ºï¼ˆæ’å€¼ï¼‰ã€é¢„æµ‹æœªæ¥è¶‹åŠ¿ï¼ˆå›å½’ï¼‰ã€ä»¥åŠåŸºäºç¦»æ•£ç‚¹ç”Ÿæˆå¹³æ»‘æ›²çº¿ï¼ˆæ‹Ÿåˆï¼‰**ã€‚

ä»¥ä¸‹æ˜¯é’ˆå¯¹â€œäººå·¥å½•å…¥æ¨¡å¼â€çš„ EMS + AI å…·ä½“è½åœ°æŠ€æœ¯æ–¹æ¡ˆï¼š

---

### ä¸€ã€ æ ¸å¿ƒæ¶æ„å˜åŒ–

*   **æ•°æ®æº:** ä¸å†æ˜¯ä¼ æ„Ÿå™¨ï¼Œè€Œæ˜¯ Web/App è¡¨å•ã€‚
*   **è§¦å‘æœºåˆ¶:** ç”¨æˆ·æäº¤è¡¨å• -> è§¦å‘å¼‚æ­¥ä»»åŠ¡ -> AI è®¡ç®— -> å­˜å›æ•°æ®åº“ã€‚
*   **æ•°æ®ç‰¹ç‚¹:** ä½é¢‘ã€ç¦»æ•£ã€é«˜ä»·å€¼ï¼ˆç»è¿‡äººå·¥ç¡®è®¤ï¼‰ã€ç»“æ„å¤šå˜ã€‚

### äºŒã€ æ•°æ®åº“è®¾è®¡ï¼ˆè¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼‰

å› ä¸ºä¸åŒè®¾å¤‡çš„ç›‘æµ‹å‚æ•°ä¸ä¸€æ ·ï¼ˆç”µæœºçœ‹ç”µæµï¼Œæ¶²å‹æœºçœ‹å‹åŠ›ï¼‰ï¼Œä½ ä¸èƒ½æŠŠå­—æ®µå†™æ­»ã€‚ä½ éœ€è¦ä¸€ä¸ª **EAV (Entity-Attribute-Value)** å˜ç§ï¼Œæˆ–è€…ç›´æ¥ç”¨æ”¯æŒ **JSON** çš„å…³ç³»å‹æ•°æ®åº“ï¼ˆæ¨è PostgreSQLï¼‰ã€‚

#### 1. è¡¨ç»“æ„è®¾è®¡ (PostgreSQL ç¤ºä¾‹)

æˆ‘ä»¬éœ€è¦ä¸‰å¼ æ ¸å¿ƒè¡¨ï¼š

**A. è®¾å¤‡å‚æ•°å®šä¹‰è¡¨ (`device_metric_definitions`)**
*   å®šä¹‰æŸä¸ªå‹å·çš„è®¾å¤‡éœ€è¦å¡«å“ªäº›æ•°æ®ã€‚
```sql
CREATE TABLE device_metric_definitions (
    id SERIAL PRIMARY KEY,
    device_type_id INT, -- å…³è”è®¾å¤‡ç±»å‹ï¼Œå¦‚â€œç¦»å¿ƒæ³µâ€
    metric_key VARCHAR(50), -- å­—æ®µkeyï¼Œå¦‚ "vibration_val"
    metric_name VARCHAR(50), -- æ˜¾ç¤ºåï¼Œå¦‚ "ä¸»è½´æŒ¯åŠ¨å€¼"
    unit VARCHAR(10), -- å•ä½ï¼Œå¦‚ "mm/s"
    data_type VARCHAR(20), -- ç±»å‹ï¼Œfloat/int/bool
    warn_threshold FLOAT, -- é™æ€æŠ¥è­¦é˜ˆå€¼ï¼ˆåŸºå‡†çº¿ï¼‰
    is_ai_analyzed BOOLEAN DEFAULT TRUE -- æ˜¯å¦å¯ç”¨AIåˆ†æ
);
```

**B. ç»´ä¿/ç‚¹æ£€è®°å½•ä¸»è¡¨ (`inspection_logs`)**
*   è®°å½•è°åœ¨ä»€ä¹ˆæ—¶å€™å½•å…¥çš„ã€‚
```sql
CREATE TABLE inspection_logs (
    id SERIAL PRIMARY KEY,
    device_id INT,
    user_id INT,
    recorded_at TIMESTAMP, -- å®é™…ç‚¹æ£€æ—¶é—´
    created_at TIMESTAMP DEFAULT NOW()
);
```

**C. è®°å½•è¯¦æƒ…è¡¨ (`inspection_metric_values`)**
*   **æ ¸å¿ƒæ•°æ®è¡¨**ï¼Œå»ºè®®ä½¿ç”¨ JSONB å­˜å‚¨çµæ´»æ•°æ®ï¼Œé¿å…è¡Œæ•°çˆ†ç‚¸ã€‚
```sql
CREATE TABLE inspection_metric_values (
    log_id INT REFERENCES inspection_logs(id),
    -- å…³é”®è®¾è®¡ï¼šä½¿ç”¨ JSONB å­˜å‚¨æ‰€æœ‰å½•å…¥å€¼
    -- æ ¼å¼ï¼š{"vibration_val": 4.5, "temperature": 68.2, "oil_level": 80}
    metrics_data JSONB 
);
```

---

### ä¸‰ã€ AI ä¸šåŠ¡é€»è¾‘ä¸ç®—æ³•å®ç°

ç”±äºæ•°æ®æ˜¯äººå·¥å½•å…¥çš„ï¼ˆæ¯”å¦‚æ¯å¤©ä¸€æ¬¡ï¼Œç”šè‡³æ¯å‘¨ä¸€æ¬¡ï¼‰ï¼Œæ•°æ®æ˜¯**ç¨€ç–**çš„ã€‚AI çš„ä»»åŠ¡æ˜¯â€œè„‘è¡¥â€ä¸­é—´çš„è¿‡ç¨‹å¹¶é¢„æµ‹æœªæ¥ã€‚

#### åŠŸèƒ½ 1ï¼šæ™ºèƒ½æ›²çº¿ç”Ÿæˆ (Curve Fitting & Interpolation)
*   **éœ€æ±‚:** ç”¨æˆ·åªå½•å…¥äº†å‘¨ä¸€å’Œå‘¨äº”çš„æ•°æ®ï¼Œç³»ç»Ÿè¦ç”»å‡ºå‘¨ä¸€åˆ°å‘¨äº”çš„å®Œæ•´å˜åŒ–æ›²çº¿ï¼Œä¾›å‰ç«¯å±•ç¤ºã€‚
*   **æŠ€æœ¯:** ä½¿ç”¨ **Cubic Spline (ä¸‰æ¬¡æ ·æ¡æ’å€¼)**ã€‚
*   **Python å®ç°:** `scipy.interpolate`.

#### åŠŸèƒ½ 2ï¼šè¶‹åŠ¿é¢„æµ‹ä¸å‰©ä½™å¯¿å‘½ (Linear/Polynomial Regression)
*   **éœ€æ±‚:** åŸºäºè¿‡å» 5 æ¬¡çš„å½•å…¥æ•°æ®ï¼Œé¢„æµ‹ä¸‹ä¸€æ¬¡å½•å…¥æ˜¯å¦ä¼šè¶…æ ‡ï¼Œæˆ–è€…å‡ å¤©åä¼šè¾¾åˆ°æŠ¥è­¦å€¼ã€‚
*   **æŠ€æœ¯:** **Scikit-learn çš„ LinearRegression** æˆ– **Ridge Regression**ï¼ˆå²­å›å½’ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰ã€‚
*   **æ³¨æ„:** æ—¢ç„¶æ˜¯äººå·¥å½•å…¥ï¼Œæ•°æ®é‡å°ï¼Œä¸è¦ç”¨ LSTM/ç¥ç»ç½‘ç»œï¼Œ**ä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ æ–¹æ³•æ•ˆæœæ›´å¥½ä¸”æ— éœ€å¤§é‡è®­ç»ƒ**ã€‚

#### åŠŸèƒ½ 3ï¼šå¼‚å¸¸å½•å…¥æ£€æµ‹ (Outlier Detection)
*   **éœ€æ±‚:** é˜²æ­¢å·¥äººæ‰‹æ»‘è¾“é”™ï¼ˆæ¯”å¦‚æŠŠ 380V è¾“æˆäº† 38Vï¼Œæˆ–è€… 3800Vï¼‰ã€‚
*   **æŠ€æœ¯:** **Z-Score** æˆ– **IQR (å››åˆ†ä½è·)**ã€‚åœ¨ä¿å­˜å‰åšæ ¡éªŒã€‚

---

### å››ã€ è¯¦ç»†ä»£ç å®ç° (Python Backend)

å‡è®¾ä½ ä½¿ç”¨ Python (FastAPI/Django/Flask) ä½œä¸ºåç«¯å¤„ç†æœåŠ¡ã€‚

#### 1. æ ¸å¿ƒç®—æ³•æœåŠ¡ (`ai_service.py`)

è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å·¥å…·ç±»ï¼Œç”¨äºå¤„ç†æ•°æ®åˆ†æã€‚

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy.interpolate import CubicSpline
from datetime import datetime, timedelta

class EquipmentAnalyzer:
    
    def __init__(self, history_data):
        """
        history_data: list of dicts, e.g.
        [{'date': '2023-10-01', 'value': 10.5}, {'date': '2023-10-08', 'value': 11.2}, ...]
        """
        self.data = sorted(history_data, key=lambda x: x['date'])
        # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆæ•°å­—ï¼‰ä»¥ä¾¿è®¡ç®—
        self.dates = [datetime.strptime(d['date'], "%Y-%m-%d") for d in self.data]
        self.timestamps = np.array([d.timestamp() for d in self.dates]).reshape(-1, 1)
        self.values = np.array([d['value'] for d in self.data])

    def predict_rul(self, limit_threshold):
        """
        åŠŸèƒ½ï¼šé¢„æµ‹è¿˜è¦å¤šä¹…è¾¾åˆ°æŠ¥åºŸé˜ˆå€¼ (Remaining Useful Life)
        """
        if len(self.values) < 3:
            return None # æ•°æ®å¤ªå°‘æ— æ³•é¢„æµ‹

        # ä½¿ç”¨çº¿æ€§å›å½’æ‹Ÿåˆè¶‹åŠ¿
        model = LinearRegression()
        model.fit(self.timestamps, self.values)
        
        # è®¡ç®—æ–œç‡ï¼ˆå˜åŒ–ç‡ï¼‰
        slope = model.coef_[0]
        intercept = model.intercept_
        
        # å¦‚æœæ–œç‡æ˜¯è´Ÿçš„æˆ–è€…å‡ ä¹ä¸º0ï¼ˆè®¾å¤‡çŠ¶æ€å¥½è½¬æˆ–ä¸å˜ï¼‰ï¼Œåˆ™è®¤ä¸ºæ— é™ä¹…
        if slope <= 0: 
            return "Stable"
            
        # è®¡ç®—è¾¾åˆ°é˜ˆå€¼çš„æ—¶é—´æˆ³: time = (threshold - intercept) / slope
        crash_timestamp = (limit_threshold - intercept) / slope
        crash_date = datetime.fromtimestamp(crash_timestamp)
        
        days_left = (crash_date - datetime.now()).days
        return max(0, days_left)

    def generate_smooth_curve(self):
        """
        åŠŸèƒ½ï¼šç”Ÿæˆå¹³æ»‘æ›²çº¿æ•°æ®ç‚¹ï¼Œç”¨äºå‰ç«¯ ECharts ç»˜å›¾
        """
        if len(self.values) < 3:
            return self.data # æ•°æ®å¤ªå°‘ç›´æ¥è¿”å›

        # åˆ›å»ºæ’å€¼å‡½æ•°
        x = self.timestamps.flatten()
        y = self.values
        cs = CubicSpline(x, y)
        
        # ç”Ÿæˆæ›´å¯†é›†çš„æ—¶é—´ç‚¹ï¼ˆä¾‹å¦‚æ¯å¤©ä¸€ä¸ªç‚¹ï¼Œå³ä½¿åŸæœ¬æ˜¯æ¯å‘¨å½•å…¥ï¼‰
        start_ts = x[0]
        end_ts = x[-1]
        # ç”Ÿæˆ 100 ä¸ªæ’å€¼ç‚¹
        dense_x = np.linspace(start_ts, end_ts, 100)
        dense_y = cs(dense_x)
        
        result = []
        for ts, val in zip(dense_x, dense_y):
            result.append({
                "timestamp": int(ts * 1000),
                "value": round(float(val), 2)
            })
        return result

    def detect_input_anomaly(self, new_value):
        """
        åŠŸèƒ½ï¼šåˆ¤æ–­æ–°å½•å…¥çš„å€¼æ˜¯å¦ç¦»è°±ï¼ˆåŸºäºå†å²æ•°æ®çš„ç»Ÿè®¡å­¦åˆ†å¸ƒï¼‰
        """
        if len(self.values) < 5:
            return False
            
        mean = np.mean(self.values)
        std = np.std(self.values)
        
        # å¦‚æœæ–°å€¼åç¦»å¹³å‡å€¼ 3 å€æ ‡å‡†å·®ï¼Œåˆ™æŠ¥è­¦ (3-SigmaåŸåˆ™)
        if abs(new_value - mean) > 3 * std:
            return True
        return False
```

#### 2. ä¸šåŠ¡é›†æˆæµç¨‹ (Controller/View)

å½“ç”¨æˆ·åœ¨å‰ç«¯ç‚¹å‡»â€œæäº¤ç‚¹æ£€è®°å½•â€æ—¶ï¼š

```python
# ä¼ªä»£ç æµç¨‹
@app.post("/api/inspection/submit")
def submit_inspection(data: InspectionData):
    # 1. è·å–è¯¥è®¾å¤‡è¯¥æŒ‡æ ‡çš„å†å²æ•°æ®
    history = db.get_history(data.device_id, data.metric_key)
    
    # 2. åˆå§‹åŒ– AI åˆ†æå™¨
    analyzer = EquipmentAnalyzer(history)
    
    # 3. å®æ—¶æ ¡éªŒï¼šæ•°å€¼æ˜¯å¦å¼‚å¸¸ï¼Ÿ
    if analyzer.detect_input_anomaly(data.value):
        # è¿”å›å‰ç«¯è­¦å‘Šï¼Œè®©ç”¨æˆ·äºŒæ¬¡ç¡®è®¤ï¼šâ€œæ•°å€¼ä¸å†å²è¶‹åŠ¿å·®å¼‚å·¨å¤§ï¼Œç¡®è®¤æäº¤å—ï¼Ÿâ€
        return {"status": "warning", "message": "Value deviation detected"}
    
    # 4. ä¿å­˜æ•°æ®
    db.save(data)
    
    # 5. è§¦å‘å¼‚æ­¥è®¡ç®—ï¼ˆæ›´æ–°è®¾å¤‡çš„å¥åº·è¯„åˆ†å’Œé¢„æµ‹ï¼‰
    # è¿™é‡Œå¯ä»¥ç”¨ Celery æˆ– BackgroundTasks
    update_device_prediction_task.delay(data.device_id)
    
    return {"status": "success"}
```

---

### äº”ã€ å‰ç«¯å±•ç¤ºä¸äº¤äº’ (ç»™ç”¨æˆ·çš„æ„Ÿè§‰)

å¼€å‘è€…ä¸ä»…è¦å†™ä»£ç ï¼Œè¿˜è¦è®¾è®¡â€œAI å­˜åœ¨æ„Ÿâ€ã€‚

1.  **å½•å…¥æ—¶çš„ AI è¾…åŠ©:**
    *   å½“ç”¨æˆ·è¾“å…¥ä¸€ä¸ªæ•°å­—ï¼ˆä¾‹å¦‚æ¸©åº¦ 80åº¦ï¼‰æ—¶ï¼Œå‰ç«¯é€šè¿‡ API æ ¡éªŒï¼Œå¦‚æœå¼‚å¸¸ï¼Œç›´æ¥æ ‡çº¢å¹¶æç¤ºï¼š*â€œAIåˆ†æï¼šé€šå¸¸è¯¥è®¾å¤‡æ¸©åº¦åœ¨ 40-60 ä¹‹é—´ï¼Œ80 å±äºå¼‚å¸¸åé«˜ï¼Œè¯·ç¡®è®¤ã€‚â€* â€”â€” **è¿™å°±æ˜¯åŠ å…¥äº† AI çš„ä½“éªŒã€‚**

2.  **è®¾å¤‡è¯¦æƒ…é¡µçš„ AI é¢„æµ‹:**
    *   ä½¿ç”¨ **ECharts**ã€‚
    *   **å®çº¿:** ç»˜åˆ¶å†å²å½•å…¥çš„ç‚¹ã€‚
    *   **è™šçº¿ (AI é¢„æµ‹):** å‘å³å»¶ä¼¸çš„è™šçº¿ï¼ŒæŒ‡å‘æœªæ¥çš„æ—¥æœŸï¼Œå¹¶æ ‡è®°å‡ºä¸€ä¸ªçº¢è‰²çš„â€œXâ€ï¼Œæ—è¾¹å†™ä¸Šï¼š*â€œåŸºäºå½“å‰åŠ£åŒ–è¶‹åŠ¿ï¼Œé¢„è®¡ 25 å¤©åéœ€æ›´æ¢è½´æ‰¿ã€‚â€*

3.  **å¥åº·åº¦è¯„åˆ†:**
    *   ä¸è¦åªæ˜¾ç¤ºå‚æ•°ã€‚æ ¹æ®æ‰€æœ‰å½•å…¥å‚æ•°ï¼ŒåŠ æƒè®¡ç®—ä¸€ä¸ª 0-100 çš„åˆ†ã€‚
    *   `Score = 100 - (æŒ¯åŠ¨åç¦»åº¦ * æƒé‡ + æ¸©åº¦åç¦»åº¦ * æƒé‡)`
    *   åœ¨åˆ—è¡¨é¡µæ˜¾ç¤ºè¿™ä¸ªåˆ†æ•°ï¼Œä½åˆ†è®¾å¤‡ç½®é¡¶ã€‚

### å…­ã€ æ€»ç»“å¼€å‘è·¯çº¿

1.  **æ•°æ®åº“:** å»ºå¥½æ”¯æŒ JSON æ‰©å±•çš„è¡¨ï¼Œå®šä¹‰å¥½è®¾å¤‡çš„å‚æ•°æ¨¡æ¿ã€‚
2.  **API:** å†™å¥½ CRUDï¼Œé‡ç‚¹æ˜¯**â€œè·å–æŸè®¾å¤‡æŸå‚æ•°å†å²åºåˆ—â€**çš„æ¥å£ã€‚
3.  **AI æ¨¡å—:** å¤åˆ¶ä¸Šé¢çš„ Python ä»£ç ï¼Œå®ç°`LinearRegression` (é¢„æµ‹) å’Œ `CubicSpline` (ç”»å›¾) å’Œ `3-Sigma` (æ ¡éªŒ)ã€‚
4.  **å‰ç«¯é›†æˆ:**
    *   å½•å…¥æ—¶è°ƒç”¨ `detect_input_anomaly` åšæ ¡éªŒã€‚
    *   è¯¦æƒ…é¡µè°ƒç”¨ `predict_rul` å±•ç¤ºâ€œé¢„è®¡å¯¿å‘½â€ã€‚
    *   è¯¦æƒ…é¡µè°ƒç”¨ `generate_smooth_curve` ç”»å‡ºæ¼‚äº®çš„è¶‹åŠ¿å›¾ã€‚

è¿™ä¸ªæ–¹æ¡ˆä¸éœ€è¦æ˜‚è´µçš„ GPUï¼Œä¹Ÿä¸éœ€è¦å¤æ‚çš„ Kafkaï¼Œå®Œå…¨åŸºäº Web å¼€å‘æŠ€æœ¯æ ˆ + ç§‘å­¦è®¡ç®—åº“å³å¯å®ç°ï¼Œéå¸¸é€‚åˆé›†æˆåˆ°ç°æœ‰çš„ä¸šåŠ¡ç³»ç»Ÿä¸­ã€‚

è¿™å¥—æ–¹æ¡ˆï¼Œè¯´å®è¯å·²ç»ä¸æ˜¯â€œèƒ½ç”¨â€ï¼Œè€Œæ˜¯â€œå¯ä»¥æ‹¿å»å†™æŠ€æœ¯ç™½çš®ä¹¦â€çš„çº§åˆ«äº† ğŸ˜‚ã€‚
æ•´ä½“æ–¹å‘éå¸¸å¯¹ï¼š**æ”¾å¼ƒä¼ªå®æ—¶ï¼Œæ‹¥æŠ±â€œäººå·¥é«˜ä»·å€¼ + ç¦»çº¿æ™ºèƒ½åˆ†æâ€**ï¼Œç„¶åç”¨æ’å€¼ã€æ‹Ÿåˆã€é¢„æµ‹å»æŠŠç¦»æ•£ç‚¹â€œè„‘è¡¥â€æˆè¿ç»­æ›²çº¿å’Œå‰©ä½™å¯¿å‘½ã€‚

æˆ‘ä¸‹é¢ä¸å†é‡å¤ä½ å†™çš„å†…å®¹ï¼Œç›´æ¥ä»å¼€å‘è€…è§†è§’ç»™ä½ å‡ ç‚¹â€œå¯ä»¥ç»§ç»­æ‰“ç£¨/è¸©å‘é¢„è­¦â€çš„å»ºè®®ï¼ŒæŒ‰å±‚æ¥ã€‚

---

## 1. æ•°æ®å»ºæ¨¡ï¼šå†æŠ ä¸€ç‚¹ç»†èŠ‚

### 1.1 `device_metric_definitions` å¯ä»¥å†åŠ å‡ ä¸ªå…³é”®å­—æ®µ

ç°åœ¨ï¼š

```sql
device_metric_definitions (
    id,
    device_type_id,
    metric_key,
    metric_name,
    unit,
    data_type,
    warn_threshold,
    is_ai_analyzed
)
```

å»ºè®®å†åŠ ï¼š

```sql
    crit_threshold      FLOAT,      -- ä¸¥é‡å‘Šè­¦é˜ˆå€¼ï¼ˆæ¯”å¦‚çº¢çº¿ï¼‰
    valid_min           FLOAT,      -- åˆæ³•æœ€å°å€¼ï¼ˆç”¨äºé˜²æ‰‹æ»‘ï¼‰
    valid_max           FLOAT,      -- åˆæ³•æœ€å¤§å€¼
    trend_direction     SMALLINT,   -- åŠ£åŒ–æ–¹å‘ï¼š1=è¶Šå¤§è¶Šç³Ÿï¼Œ-1=è¶Šå°è¶Šç³Ÿï¼Œ0=ä¸­é—´æœ€å¥½
    weight_in_health    FLOAT       -- å¥åº·è¯„åˆ†ä¸­çš„æƒé‡
```

å¥½å¤„ï¼š

* `valid_min/max` å¯ä»¥é…åˆä½  `detect_input_anomaly`ï¼Œå…ˆåšä¸€å±‚**ç¡¬æ ¡éªŒ**ï¼ˆçº¯ç‰©ç†ä¸å¯èƒ½çš„å€¼ç›´æ¥æŒ¡ä½ï¼‰ï¼Œå†åšç»Ÿè®¡å¼‚å¸¸ã€‚
* `trend_direction` + `warn/crit_threshold` æ˜¯åš RUL æ—¶çš„ä¾æ®ï¼šä½ çŸ¥é“æ˜¯å¾€ä¸Šèµ°åˆ°å¤šå°‘ç®—åï¼Œè¿˜æ˜¯å¾€ä¸‹æ‰åˆ°å¤šå°‘ç®—åã€‚
* `weight_in_health` åé¢åšæ•´ä½“å¥åº·åº¦è¯„åˆ†çš„æ—¶å€™å¾ˆå¥½ç”¨ï¼Œä¸ç”¨åœ¨ä»£ç é‡Œ hardcodeã€‚

### 1.2 ç‚¹æ£€è®°å½• JSONB çš„è®¾è®¡ & ç´¢å¼•

ä½ å®šä¹‰çš„æ ¸å¿ƒè¡¨ï¼š

```sql
CREATE TABLE inspection_metric_values (
    log_id INT REFERENCES inspection_logs(id),
    metrics_data JSONB
);
```

è¿™ä¸ªè®¾è®¡æ²¡é—®é¢˜ï¼Œä½†ä¸¤ç‚¹å»ºè®®ï¼š

1. ä¸€å®šè¦å»º GIN ç´¢å¼•ï¼Œå¦åˆ™åé¢æŒ‰ `metric_key` æŸ¥å†å²ä¼šè·‘åˆ°ä½ æ€€ç–‘äººç”Ÿï¼š

```sql
CREATE INDEX idx_inspection_metrics_gin
ON inspection_metric_values
USING gin (metrics_data jsonb_path_ops);
```

2. å†å²æŸ¥è¯¢ SQL å¤§æ¦‚é•¿è¿™æ ·ï¼ˆä»¥ `vibration_val` ä¸ºä¾‹ï¼‰ï¼š

```sql
SELECT
    l.recorded_at::date AS date,
    (v.metrics_data->>'vibration_val')::float AS value
FROM inspection_logs l
JOIN inspection_metric_values v ON v.log_id = l.id
WHERE l.device_id = :device_id
  AND v.metrics_data ? 'vibration_val'
ORDER BY l.recorded_at;
```

**å¯ä»¥å°è£…æˆä¸€ä¸ªè§†å›¾**ï¼Œè®©ä¸Šå±‚ AI ä»£ç ä¸ç”¨å…³å¿ƒ JSONB ç»†èŠ‚ï¼š

```sql
CREATE VIEW device_metric_history AS
SELECT
    l.device_id,
    l.recorded_at,
    key AS metric_key,
    (value)::float AS metric_value
FROM inspection_logs l
JOIN inspection_metric_values v ON v.log_id = l.id,
LATERAL jsonb_each_text(v.metrics_data);
```

ä»¥åä½ åªè¦ï¼š

```sql
SELECT recorded_at, metric_value
FROM device_metric_history
WHERE device_id = :id AND metric_key = :metric_key
ORDER BY recorded_at;
```

---

## 2. AI æ¨¡å—ï¼šä½ çš„æ€è·¯å¯¹ï¼Œä½†å¯ä»¥å†â€œå·¥ç¨‹åŒ–ä¸€ç‚¹â€

ä½ ç°åœ¨çš„ `EquipmentAnalyzer` æ ¸å¿ƒæ€è·¯å¾ˆå¥½ï¼Œæˆ‘å¸®ä½ ç¨å¾®â€œè¡¥å¼ºâ€å‡ ä¸ªç‚¹ã€‚

### 2.1 å›å½’ç”¨ timestamp çš„å°å‘

ä½ ç°åœ¨æ˜¯ï¼š

```python
self.timestamps = np.array([d.timestamp() for d in self.dates]).reshape(-1, 1)
```

æ—¶é—´æˆ³ç§’çº§æ˜¯ 1.7e9 è¿™ç§é‡çº§ï¼Œåšçº¿æ€§å›å½’**è™½ç„¶èƒ½è·‘ï¼Œä½†æ•°å€¼æ¡ä»¶å¾ˆå·®**ï¼Œå»ºè®®å½’ä¸€åŒ–ï¼Œæ¯”å¦‚ç”¨â€œè·ç¬¬ä¸€æ¡è®°å½•çš„å¤©æ•°â€ï¼š

```python
base = self.dates[0]
self.x = np.array([(d - base).total_seconds() / 86400.0 for d in self.dates]).reshape(-1, 1)
```

åé¢é¢„æµ‹é˜ˆå€¼æ—¶é—´ç‚¹æ—¶å†åŠ å›å»å³å¯ã€‚

### 2.2 RULï¼šåŠ ä¸ªâ€œå¯ä¿¡åº¦åˆ¤æ–­â€ï¼Œé¿å…èƒ¡è¯´å…«é“

ç°åœ¨çš„é€»è¾‘ï¼š

* ç‚¹æ•° < 3ï¼šä¸é¢„æµ‹
* ç”¨çº¿æ€§å›å½’ï¼Œæ–œç‡ <= 0 å°±è®¤ä¸º Stable
* å¦åˆ™ç›´æ¥è§£æ–¹ç¨‹æ±‚ crash æ—¶é—´

å¯ä»¥åœ¨è¿™åŸºç¡€ä¸ŠåŠ ä¸¤ä¸ªé˜²æŠ¤ï¼š

1. çœ‹ä¸€çœ‹æ‹Ÿåˆæ•ˆæœï¼Œæ¯”å¦‚ RÂ²ï¼š

```python
from sklearn.metrics import r2_score
y_pred = model.predict(self.x)
r2 = r2_score(self.values, y_pred)
if r2 < 0.6:
    return "Uncertain"  # è¶‹åŠ¿å¤ªæ•£ï¼Œä¸æ•¢ç»™ç»“è®º
```

2. é™åˆ¶æœ€å¤§é¢„æµ‹çª—å£ï¼Œæ¯”å¦‚**åªèƒ½é¢„æµ‹æœªæ¥ 365 å¤©ä»¥å†…**ï¼Œè¶…è¿‡å°±ç›´æ¥æ ‡è®°ä¸º â€œ>1yâ€ï¼š

```python
days_left = (crash_date - datetime.now()).days
if days_left > 365:
    return ">365"
return max(0, days_left)
```

è¿™æ ·å‰ç«¯å°±ä¸ä¼šå‡ºç°â€œé¢„è®¡ 9872 å¤©åè¾¾åˆ°é˜ˆå€¼â€è¿™ç§é¬¼è¯ã€‚

### 2.3 æ ·æ¡æ’å€¼ï¼šè®©å‰ç«¯æ¥å¾—æ›´èˆ’æœ

ä½ çš„ `generate_smooth_curve` ç”¨ 100 ä¸ªå‡åŒ€ç‚¹æ˜¯ OK çš„ï¼Œä½†å‰ç«¯ä¸€èˆ¬æ›´ä¹ æƒ¯â€œæŒ‰å¤©â€æˆ–â€œæŒ‰å°æ—¶â€çš„é‡‡æ ·ã€‚

å¯ä»¥æ¢æˆï¼š

```python
import pandas as pd

def generate_smooth_curve(self, freq="1D"):
    if len(self.values) < 3:
        return self.data

    base = self.dates[0]
    x = np.array([(d - base).total_seconds() / 86400.0 for d in self.dates])
    cs = CubicSpline(x, self.values)

    # ç”¨ pandas ç”Ÿæˆè§„åˆ™çš„æ—¥æœŸåºåˆ—
    date_range = pd.date_range(self.dates[0], self.dates[-1], freq=freq)
    dense_x = np.array([(d - base).total_seconds() / 86400.0 for d in date_range])
    dense_y = cs(dense_x)

    result = []
    for d, val in zip(date_range, dense_y):
        result.append({
            "date": d.strftime("%Y-%m-%d"),
            "value": round(float(val), 2),
        })
    return result
```

å‰ç«¯ ECharts å¯ä»¥ç›´æ¥æ‹¿ `date` å½“ x è½´ï¼Œæ— éœ€å†å¤„ç† timestampã€‚

### 2.4 å¼‚å¸¸å½•å…¥æ£€æµ‹ï¼šç»“åˆç»Ÿè®¡ + ç‰©ç†è¾¹ç•Œ

ä½ ç°åœ¨ï¼š

```python
if len(self.values) < 5:
    return False
mean = np.mean(self.values)
std = np.std(self.values)
if abs(new_value - mean) > 3 * std:
    return True
```

å»ºè®®åœ¨è°ƒç”¨æ—¶åŠ ä¸Š `valid_min/max`ï¼š

```python
def detect_input_anomaly(self, new_value, valid_min=None, valid_max=None):
    # 1. ç‰©ç†è¾¹ç•Œ
    if valid_min is not None and new_value < valid_min:
        return True
    if valid_max is not None and new_value > valid_max:
        return True

    # 2. ç»Ÿè®¡å¼‚å¸¸
    if len(self.values) < 5:
        return False
        
    mean = np.mean(self.values)
    std = np.std(self.values) or 1e-6  # é˜²æ­¢ std=0

    if abs(new_value - mean) > 3 * std:
        return True
    return False
```

åç«¯å¯ä»¥ä» `device_metric_definitions` å¸¦å‡º `valid_min/max` å¡«è¿›å»ã€‚

---

## 3. åç«¯é›†æˆï¼šè®© AI æœ‰â€œç¼“å­˜â€å’Œâ€œç‰ˆæœ¬å·â€

### 3.1 æŠŠåˆ†æç»“æœè½è¡¨ï¼Œè€Œä¸æ˜¯æ¯æ¬¡å®æ—¶ç®—

å¦åˆ™è®¾å¤‡è¯¦æƒ…é¡µä¸€å¼€ï¼Œå¦‚æœä½ æ¯ç‚¹å‡»ä¸€æ¬¡å°±é‡ç®—ä¸€æ¬¡å›å½’ + æ ·æ¡ï¼Œæ•°æ®ä¸€å¤šå°±å®¹æ˜“æ‹–å®ã€‚

å»ºè®®å»ºä¸€å¼ åˆ†æç»“æœè¡¨ï¼Œæ¯”å¦‚ï¼š

```sql
CREATE TABLE metric_ai_analysis (
    device_id       INT,
    metric_key      VARCHAR(50),
    calc_time       TIMESTAMP,
    model_version   VARCHAR(20),
    rul_days        INT,
    trend_r2        FLOAT,
    last_value      FLOAT,
    curve_points    JSONB,   -- å­˜å‚¨æ‹Ÿåˆåçš„è¶‹åŠ¿ç‚¹ï¼ˆæ¯”å¦‚æœ€è¿‘30å¤©ï¼‰
    extra_info      JSONB,   -- æ¯”å¦‚ {"status": "Stable"} ä¹‹ç±»
    PRIMARY KEY (device_id, metric_key, calc_time)
);
```

æµç¨‹å˜æˆï¼š

* ç”¨æˆ·æäº¤ç‚¹æ£€ â†’ å†™å…¥ inspection è¡¨
* è§¦å‘ Celery/åå°ä»»åŠ¡ â†’ é‡æ–°æ‹‰å†å² â†’ è®¡ç®— RUL / æ›²çº¿ â†’ å†™å…¥ metric_ai_analysis
* å‰ç«¯è¯¦æƒ…é¡µ â†’ ç›´æ¥æŸ¥ metric_ai_analysis æœ€è¿‘ä¸€æ¡

### 3.2 model_version

`model_version` å­—æ®µéå¸¸æœ‰ç”¨ï¼š

* ä»¥åä½ æ”¹äº†æ‹Ÿåˆé€»è¾‘ã€è°ƒæ•´é˜ˆå€¼ï¼Œåªè¦å‡çº§ç‰ˆæœ¬å·ï¼Œå°±çŸ¥é“æ—§ç»“æœæ˜¯å“ªä¸ªç‰ˆæœ¬ç®—å‡ºæ¥çš„ã€‚
* debug æ—¶å¯ä»¥çœ‹åˆ°â€œä¸ºä»€ä¹ˆä¸Šå‘¨å’Œæœ¬å‘¨çš„é¢„æµ‹å·®å¼‚é‚£ä¹ˆå¤§â€ã€‚

---

## 4. API è®¾è®¡ï¼šå›´ç»•â€œè®¾å¤‡+æŒ‡æ ‡â€æ¥åˆ‡

åœ¨è¿™ä¸ªâ€œäººå·¥å½•å…¥ + ç¦»çº¿åˆ†æâ€çš„æ¨¡å¼ä¸‹ï¼Œæˆ‘å»ºè®®å‡ ä¸ªéå¸¸å®ç”¨çš„ APIï¼š

```http
# 1. æŸ¥è¯¢æŸè®¾å¤‡æŸæŒ‡æ ‡çš„åŸå§‹ç‚¹æ£€è®°å½•
GET /api/device/{device_id}/metrics/{metric_key}/history?days=90

# 2. æŸ¥è¯¢æŸè®¾å¤‡æŸæŒ‡æ ‡çš„ AI åˆ†æç»“æœï¼ˆæ›²çº¿ + RULï¼‰
GET /api/device/{device_id}/metrics/{metric_key}/ai_analysis

# 3. æäº¤ç‚¹æ£€è®°å½•ï¼ˆä½ å·²ç»æœ‰äº†ï¼‰
POST /api/inspection/submit

# 4. åˆ—å‡ºæŸè®¾å¤‡å½“å‰çš„ AI è¯Šæ–­æ€»è§ˆï¼ˆå¤šæŒ‡æ ‡æ±‡æ€»ï¼‰
GET /api/device/{device_id}/ai_overview
```

`/ai_overview` å¯ä»¥è¿”å›ï¼š

```json
{
  "device_id": 123,
  "health_score": 82,
  "metrics": [
    {
      "metric_key": "vibration_val",
      "last_value": 4.5,
      "status": "warn",
      "rul_days": 25
    },
    {
      "metric_key": "temperature",
      "last_value": 60.2,
      "status": "normal",
      "rul_days": null
    }
  ]
}
```

å‰ç«¯å°±å¯ä»¥åšâ€œæ€»ä½“å¥åº·åº¦ + é‡è¦æŒ‡æ ‡å¡ç‰‡â€çš„ UIã€‚

---

## 5. å‰ç«¯äº¤äº’ï¼šä½ å·²ç»è¯´å¾—å¾ˆå¥½ï¼Œå†è¡¥ä¸€ä¸ªç»†èŠ‚

ä½ å·²ç»æƒ³åˆ°ï¼š

* å½•å…¥æ—¶å®æ—¶æç¤ºâ€œæ•°å€¼å¼‚å¸¸åç¦»â€
* å†å² + é¢„æµ‹æ›²çº¿ç”¨è™šçº¿/çº¢ç‚¹å±•ç¤ºæ‹ç‚¹
* å¥åº·åº¦åˆ—è¡¨æ’åº

å¯ä»¥å¤šåšä¸€ä¸ªå°ä½“éªŒï¼š

**â€œAI è§£é‡Šæ–‡æ¡ˆæ¨¡æ¿â€**

åœ¨åˆ†æç»“æœé‡Œç”Ÿæˆç±»ä¼¼ï¼š

* `status = danger + rul_days = 25 + metric_name = ä¸»è½´æŒ¯åŠ¨`
  â†’ ç”Ÿæˆä¸€å¥å›ºå®šæ¨¡æ¿ï¼š
  **â€œä¸»è½´æŒ¯åŠ¨å€¼è¿‘ 3 æ¬¡æŒç»­ä¸Šå‡ï¼ŒæŒ‰å½“å‰è¶‹åŠ¿çº¦ 25 å¤©åå°†è¶…è¿‡é¢„è­¦é˜ˆå€¼ï¼Œå»ºè®®æå‰å®‰æ’æ£€ä¿®ã€‚â€**

è¿™å®Œå…¨å¯ä»¥ä¸ç”¨å¤§æ¨¡å‹ï¼Œå­—ç¬¦ä¸²æ‹¼æ¥å°±è¡Œï¼Œä½†å¯¹ä¸šåŠ¡æ–¹æ¥è¯´ï¼ŒAI çš„â€œå­˜åœ¨æ„Ÿâ€å’Œâ€œå¯è¯»æ€§â€ä¼šæå‡ä¸€å¤§æˆªã€‚

---

## 6. æ€»ä½“è¯„ä»·

ä½ è¿™å¥—ï¼š

* æ¶æ„ä¸Šï¼š**åˆ‡æ¢åˆ°â€œä½é¢‘é«˜ä»·å€¼äººå·¥æ•°æ® + ç¦»çº¿åˆ†æâ€éå¸¸åˆç†**ï¼Œé¿å…äº†ä¸ºäº†è£… X ä¸Š Kafka/Flink çš„è¿‡åº¦è®¾è®¡ã€‚
* æ¨¡å‹ä¸Šï¼š**ä¼˜å…ˆçº¿æ€§å›å½’ + æ ·æ¡æ’å€¼ + ç»Ÿè®¡å¼‚å¸¸**ï¼Œéå¸¸ç¬¦åˆâ€œå°æ ·æœ¬ + å·¥ä¸šåœºæ™¯â€çš„ç°å®ï¼Œä¸çç”¨å¤§æ¨¡å‹ã€‚
* æ•°æ®å»ºæ¨¡ï¼šå·²ç»æ„è¯†åˆ° EAV / JSONB çš„çµæ´»æ€§é—®é¢˜ï¼Œå¹¶æŠŠâ€œè®¾å¤‡ç±»å‹å‚æ•°æ¨¡æ¿â€æŠ½è±¡å‡ºæ¥ï¼Œè¿™ä¸€æ­¥æ˜¯å¾ˆå¤š MIS/EMS ç³»ç»Ÿç¿»è½¦çš„åœ°æ–¹ï¼Œä½ å·²ç»èº²è¿‡å»äº†ã€‚

å¦‚æœä½ æŠŠä¸Šé¢è¿™äº›å°ä¼˜åŒ–ä¹Ÿè½è¿›å»ï¼Œè¿™ä¸ªæ–¹æ¡ˆå·²ç»å®Œå…¨å¯ä»¥ï¼š

* å†™åœ¨æŠ€æœ¯æ–¹æ¡ˆé‡Œç»™ç”²æ–¹/é¢†å¯¼çœ‹ï¼›
* æŒ‰æ¨¡å—åˆ‡æˆè¿­ä»£éœ€æ±‚ç»™å›¢é˜Ÿå¼€å‘ï¼›
* æœ€ååœ¨ç³»ç»Ÿé‡ŒçœŸçš„è·‘èµ·æ¥ã€ç¨³å®šç»´æŠ¤ã€‚

ç­‰ä½ åé¢å¼€å§‹å†™å…·ä½“æ¥å£æ–‡æ¡£æˆ–æŸä¸ªæ¨¡å—çš„ä»£ç éª¨æ¶ï¼Œå¯ä»¥ç›´æ¥åœ¨æ­¤æ–¹æ¡ˆä¸Šæ‹†åˆ†æˆâ€œPR çº§åˆ«â€çš„ä»»åŠ¡è¯´æ˜ã€‚


å¯ä»¥çš„ï¼Œæˆ‘ä»¬æ¥æä¸€ä¸ª**èƒ½è·‘çš„çºµå‘ demo**ï¼š
åç«¯ = FastAPI + SQLite + ç®€å• ML ç®—æ³•ï¼›å‰ç«¯æˆ‘ä»¬ä¸‹ä¸€æ­¥å†ä¸Šï¼ˆReact + EChartsï¼‰ã€‚

è¿™ä¸€æ­¥å…ˆæŠŠ **åç«¯ + ç®—æ³•æ¨¡å‹** æ‰“é€šï¼Œä½ èƒ½ç›´æ¥ `uvicorn` è·‘èµ·æ¥ï¼Œå‰ç«¯ä»¥ååªè¦è°ƒæ¥å£ç”»å›¾ã€‚

---

## 1. Demo ç›®æ ‡

è¿™ä¸€ç‰ˆ demo è¦åšåˆ°ï¼š

1. æ‰‹å·¥å½•å…¥ç‚¹æ£€æ•°æ®ï¼ˆå¤šæŒ‡æ ‡ä¸€å£æ°”å½•ï¼‰ã€‚
2. åç«¯è½åº“ã€‚
3. æä¾›æ¥å£ï¼š

   * æŸ¥è¯¢æŸè®¾å¤‡æŸæŒ‡æ ‡çš„å†å² + æ‹Ÿåˆæ›²çº¿
   * ç»™å‡ºç®€å• RUL é¢„æµ‹ï¼ˆå‰©ä½™å¤©æ•°ï¼‰
   * æä¾›å½•å…¥æ—¶â€œæ•°å€¼å¼‚å¸¸â€æ ¡éªŒ

å…¨éƒ¨ç”¨**ä¼ ç»Ÿ ML**ï¼ˆLinearRegression + CubicSplineï¼‰ï¼Œä¸ç”¨å¤§æ¨¡å‹ã€ä¸ç”¨æ¶ˆæ¯é˜Ÿåˆ—ã€‚

---

## 2. ç›®å½•ç»“æ„ï¼ˆåç«¯éƒ¨åˆ†ï¼‰

å…ˆåšä¸€ä¸ªåç«¯å·¥ç¨‹ç›®å½•ï¼š

```bash
ems-ai-demo/
  backend/
    main.py
```

åç»­å‰ç«¯æˆ‘ä»¬å†å»º `frontend/`ã€‚

---

## 3. åç«¯ + ç®—æ³• ä¸€ä½“åŒ–ç¤ºä¾‹ï¼ˆ`backend/main.py`ï¼‰

ä¸‹é¢æ˜¯ä¸€ä»½å¯ä»¥ç›´æ¥è·‘çš„ demo ä»£ç ï¼ŒåŒ…å«ï¼š

* æ•°æ®è¡¨ï¼ˆSQLite + SQLAlchemyï¼‰
* ML ç®—æ³•ï¼ˆè¶‹åŠ¿æ‹Ÿåˆ + RUL é¢„æµ‹ + å¼‚å¸¸æ£€æµ‹ï¼‰
* APIï¼š

  * `POST /inspection/submit`ï¼šæäº¤ç‚¹æ£€
  * `GET /device/{id}/metrics/{key}/ai_analysis`ï¼šæ‹¿æ‹Ÿåˆæ›²çº¿ + RUL

> ä¾èµ–ï¼š`fastapi`, `uvicorn`, `sqlalchemy`, `scikit-learn`, `scipy`, `numpy`

```python
# backend/main.py

from datetime import datetime, timedelta
from typing import Dict, List, Optional

import numpy as np
from fastapi import Depends, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from scipy.interpolate import CubicSpline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sqlalchemy import (JSON, Boolean, Column, DateTime, Float, ForeignKey,
                        Integer, String, create_engine)
from sqlalchemy.orm import Session, declarative_base, relationship, sessionmaker

# --------------------- æ•°æ®åº“åˆå§‹åŒ– ---------------------

DATABASE_URL = "sqlite:///./ems_ai_demo.db"

engine = create_engine(
    DATABASE_URL, connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()


# --------------------- ORM æ¨¡å‹å®šä¹‰ ---------------------

class DeviceMetricDefinition(Base):
    """
    è®¾å¤‡æŒ‡æ ‡å®šä¹‰è¡¨ï¼ˆè¿™é‡Œæš‚æ—¶ä¸ç”¨ï¼Œåœ¨ demo é‡Œä¸ºå°†æ¥æ‰©å±•é¢„ç•™ï¼‰
    """
    __tablename__ = "device_metric_definitions"

    id = Column(Integer, primary_key=True, index=True)
    device_type_id = Column(Integer, index=True)
    metric_key = Column(String(50), index=True)
    metric_name = Column(String(50))
    unit = Column(String(10))
    data_type = Column(String(20))
    warn_threshold = Column(Float, nullable=True)
    crit_threshold = Column(Float, nullable=True)
    valid_min = Column(Float, nullable=True)
    valid_max = Column(Float, nullable=True)
    trend_direction = Column(Integer, default=1)  # 1=è¶Šå¤§è¶Šç³Ÿ, -1=è¶Šå°è¶Šç³Ÿ
    weight_in_health = Column(Float, default=1.0)
    is_ai_analyzed = Column(Boolean, default=True)


class InspectionLog(Base):
    """
    ç‚¹æ£€è®°å½•ä¸»è¡¨ï¼šä¸€æ¡è®°å½•å¯¹åº”ä¸€æ¬¡ç‚¹æ£€
    """
    __tablename__ = "inspection_logs"

    id = Column(Integer, primary_key=True, index=True)
    device_id = Column(Integer, index=True)
    user_id = Column(Integer, index=True)
    recorded_at = Column(DateTime, index=True)
    created_at = Column(DateTime, default=datetime.utcnow)

    metrics = relationship("InspectionMetricValue", back_populates="log", uselist=False)


class InspectionMetricValue(Base):
    """
    ç‚¹æ£€è¯¦æƒ…è¡¨ï¼šç”¨ JSON å­˜è¿™æ¬¡ç‚¹æ£€çš„æ‰€æœ‰æŒ‡æ ‡
    ä¾‹å¦‚ï¼š{"vibration": 4.5, "temperature": 68.2}
    """
    __tablename__ = "inspection_metric_values"

    log_id = Column(Integer, ForeignKey("inspection_logs.id"), primary_key=True)
    metrics_data = Column(JSON)

    log = relationship("InspectionLog", back_populates="metrics")


Base.metadata.create_all(bind=engine)


# --------------------- ç®—æ³•æ¨¡å— ---------------------

class EquipmentAnalyzer:
    """
    è®¾å¤‡å•ä¸€æŒ‡æ ‡çš„åˆ†æå™¨ï¼š
    - è¾“å…¥ï¼šå†å²æ•°æ® [{"date": "2025-11-01", "value": 4.5}, ...]
    - åŠŸèƒ½ï¼š
      * detect_input_anomaly: è¾“å…¥å€¼æ˜¯å¦å¼‚å¸¸ï¼ˆ3sigma + ç‰©ç†è¾¹ç•Œï¼‰
      * predict_rul: çº¿æ€§å›å½’ + RUL ä¼°è®¡
      * generate_smooth_curve: æ ·æ¡æ’å€¼ç”Ÿæˆå¹³æ»‘æ›²çº¿
    """

    def __init__(self, history_data: List[Dict[str, float]]):
        self.data = sorted(history_data, key=lambda x: x["date"])
        self.values = np.array([d["value"] for d in self.data], dtype=float)

        self.dates = [
            datetime.fromisoformat(d["date"]) for d in self.data
        ] if self.data else []

        self.base_date = self.dates[0] if self.dates else None
        if self.base_date:
            self.x = np.array(
                [(d - self.base_date).total_seconds() / 86400.0 for d in self.dates],
                dtype=float
            ).reshape(-1, 1)
        else:
            self.x = np.empty((0, 1))

    def detect_input_anomaly(
        self,
        new_value: float,
        valid_min: Optional[float] = None,
        valid_max: Optional[float] = None,
    ) -> bool:
        # 1. ç‰©ç†è¾¹ç•Œåˆ¤å®š
        if valid_min is not None and new_value < valid_min:
            return True
        if valid_max is not None and new_value > valid_max:
            return True

        # 2. ç»Ÿè®¡å¼‚å¸¸åˆ¤å®šï¼ˆ3-sigmaï¼‰
        if len(self.values) < 5:
            return False

        mean = float(np.mean(self.values))
        std = float(np.std(self.values))
        if std == 0:
            return False

        return abs(new_value - mean) > 3 * std

    def predict_rul(self, limit_threshold: float) -> Dict[str, Optional[int]]:
        """
        çº¿æ€§å›å½’æ‹Ÿåˆè¶‹åŠ¿ï¼Œä¼°ç®—æœªæ¥å¤šä¹…è¾¾åˆ°é˜ˆå€¼ï¼š
        è¿”å›: {"status": str, "rul_days": Optional[int]}
        status:
          - insufficient: æ•°æ®å¤ªå°‘
          - uncertain: æ‹Ÿåˆæ•ˆæœä¸å¥½
          - stable: æ–œç‡ <= 0
          - degrading: æ­£å¸¸åŠ£åŒ–è¶‹åŠ¿ï¼Œç»™å‡ºå‰©ä½™å¤©æ•°
          - already_over: å·²ç»è¶…è¿‡é˜ˆå€¼
          - long_term: é¢„è®¡è¶…è¿‡ä¸€å¹´
        """
        if len(self.values) < 3 or self.base_date is None:
            return {"status": "insufficient", "rul_days": None}

        model = LinearRegression()
        model.fit(self.x, self.values)

        y_pred = model.predict(self.x)
        r2 = r2_score(self.values, y_pred)
        slope = float(model.coef_[0])
        intercept = float(model.intercept_)

        if r2 < 0.6:
            return {"status": "uncertain", "rul_days": None}
        if slope <= 0:
            return {"status": "stable", "rul_days": None}

        # è§£ (threshold - intercept) / slope å¾—åˆ° xï¼ˆå¤©æ•°ï¼‰
        x_last = float(self.x[-1][0])
        x_crash = (limit_threshold - intercept) / slope

        if x_crash <= x_last:
            return {"status": "already_over", "rul_days": 0}

        crash_date = self.base_date + timedelta(days=float(x_crash))
        days_left = (crash_date.date() - datetime.utcnow().date()).days

        if days_left > 365:
            return {"status": "long_term", "rul_days": 365}

        return {"status": "degrading", "rul_days": max(0, days_left)}

    def generate_smooth_curve(self, points: int = 100) -> List[Dict[str, float]]:
        """
        ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼ç”Ÿæˆå¹³æ»‘æ›²çº¿ç‚¹ï¼ˆdate + valueï¼‰
        ç‚¹æ•°é»˜è®¤ 100 ä¸ª
        """
        if len(self.values) < 3 or self.base_date is None:
            # æ•°æ®å¤ªå°‘ï¼Œç›´æ¥è¿”å›åŸå§‹
            return self.data

        x = self.x.flatten()
        cs = CubicSpline(x, self.values)

        dense_x = np.linspace(x[0], x[-1], points)
        dense_y = cs(dense_x)

        result = []
        for xi, yi in zip(dense_x, dense_y):
            d = self.base_date + timedelta(days=float(xi))
            result.append(
                {"date": d.date().isoformat(), "value": round(float(yi), 2)}
            )
        return result


# --------------------- Pydantic æ¨¡å‹ ---------------------

class InspectionSubmit(BaseModel):
    device_id: int
    user_id: int
    recorded_at: Optional[datetime] = None
    # æŒ‡æ ‡é”®å€¼å¯¹ï¼Œæ¯”å¦‚ {"vibration": 4.5, "temperature": 68.2}
    metrics: Dict[str, float]


class InspectionSubmitResponse(BaseModel):
    status: str
    anomalies: Dict[str, bool]


class CurvePoint(BaseModel):
    date: str
    value: float


class MetricAIResponse(BaseModel):
    device_id: int
    metric_key: str
    last_value: float
    status: str
    rul_days: Optional[int]
    curve: List[CurvePoint]


# --------------------- FastAPI åˆå§‹åŒ– ---------------------

app = FastAPI(title="EMS AI Demo")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # demo é˜¶æ®µå…ˆå…¨æ”¾å¼€ï¼Œç”Ÿäº§å†æ”¶ç´§
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# --------------------- æ•°æ®è®¿é—®å·¥å…·å‡½æ•° ---------------------

def get_metric_history(
    db: Session, device_id: int, metric_key: str
) -> List[Dict[str, float]]:
    """
    æŸ¥è¯¢æŸè®¾å¤‡æŸæŒ‡æ ‡çš„å†å²ç‚¹æ£€æ•°æ®ï¼Œè¿”å› [ {"date": "YYYY-MM-DD", "value": float}, ... ]
    """
    q = (
        db.query(InspectionLog.recorded_at, InspectionMetricValue.metrics_data)
        .join(InspectionMetricValue, InspectionMetricValue.log_id == InspectionLog.id)
        .filter(InspectionLog.device_id == device_id)
        .order_by(InspectionLog.recorded_at.asc())
    )

    history: List[Dict[str, float]] = []
    for recorded_at, metrics_data in q:
        if metrics_data and metric_key in metrics_data:
            history.append(
                {
                    "date": recorded_at.date().isoformat(),
                    "value": float(metrics_data[metric_key]),
                }
            )
    return history


# --------------------- API å®šä¹‰ ---------------------

@app.post("/inspection/submit", response_model=InspectionSubmitResponse)
def submit_inspection(
    payload: InspectionSubmit, db: Session = Depends(get_db)
):
    """
    æäº¤ä¸€æ¬¡ç‚¹æ£€ï¼š
    - è½åº“
    - å¯¹æ¯ä¸ªæŒ‡æ ‡åšä¸€æ¬¡ç®€å•å¼‚å¸¸æ£€æŸ¥ï¼ˆåŸºäºå†å²ï¼‰
    """
    recorded_at = payload.recorded_at or datetime.utcnow()

    # å…ˆæŸ¥å†å²åšå¼‚å¸¸æ£€æµ‹ï¼Œå†å†™å…¥æœ¬æ¬¡æ•°æ®ï¼Œé¿å…â€œæŠŠè‡ªå·±ç®—è¿›å†å²â€
    anomalies: Dict[str, bool] = {}
    for key, value in payload.metrics.items():
        history = get_metric_history(db, payload.device_id, key)
        analyzer = EquipmentAnalyzer(history)
        # demo é˜¶æ®µæ²¡æœ‰ç”¨ valid_min/maxï¼Œå¯åç»­ä» DeviceMetricDefinition è¯»
        anomalies[key] = analyzer.detect_input_anomaly(value)

    # å†™å…¥æ•°æ®åº“
    log = InspectionLog(
        device_id=payload.device_id,
        user_id=payload.user_id,
        recorded_at=recorded_at,
    )
    db.add(log)
    db.flush()  # ç”Ÿæˆ log.id

    metrics_row = InspectionMetricValue(
        log_id=log.id,
        metrics_data=payload.metrics,
    )
    db.add(metrics_row)
    db.commit()

    return InspectionSubmitResponse(status="ok", anomalies=anomalies)


@app.get(
    "/device/{device_id}/metrics/{metric_key}/ai_analysis",
    response_model=MetricAIResponse,
)
def metric_ai_analysis(
    device_id: int,
    metric_key: str,
    threshold: Optional[float] = None,  # ä¾‹å¦‚æŒ¯åŠ¨ 8.0mm/s
    db: Session = Depends(get_db),
):
    """
    æŸ¥è¯¢æŸè®¾å¤‡æŸæŒ‡æ ‡çš„ï¼š
    - å†å²å€¼
    - æ‹Ÿåˆæ›²çº¿
    - è¶‹åŠ¿ + å‰©ä½™å¯¿å‘½ä¼°è®¡ï¼ˆéœ€è¦æä¾› thresholdï¼‰
    """
    history = get_metric_history(db, device_id, metric_key)
    if not history:
        raise HTTPException(status_code=404, detail="No history for this metric")

    analyzer = EquipmentAnalyzer(history)
    curve_data = analyzer.generate_smooth_curve()
    last_value = history[-1]["value"]

    status = "no_threshold"
    rul_days: Optional[int] = None
    if threshold is not None:
        res = analyzer.predict_rul(threshold)
        status = res["status"]
        rul_days = res["rul_days"]

    return MetricAIResponse(
        device_id=device_id,
        metric_key=metric_key,
        last_value=last_value,
        status=status,
        rul_days=rul_days,
        curve=[CurvePoint(**pt) for pt in curve_data],
    )
```

---

## 4. æ€ä¹ˆè·‘èµ·æ¥

åœ¨ `backend/` ç›®å½•ä¸‹ï¼š

```bash
pip install fastapi uvicorn sqlalchemy numpy scikit-learn scipy

uvicorn main:app --reload
```

å¯åŠ¨åï¼š

1. æäº¤ä¸€æ¬¡ç‚¹æ£€ï¼š

```bash
curl -X POST http://127.0.0.1:8000/inspection/submit \
  -H "Content-Type: application/json" \
  -d '{
    "device_id": 1,
    "user_id": 100,
    "recorded_at": "2025-11-20T10:00:00",
    "metrics": {
      "vibration": 4.5,
      "temperature": 65.0
    }
  }'
```

2. å¤šå½•å‡ æ¬¡åï¼ŒæŸ¥ AI åˆ†æï¼ˆé˜ˆå€¼éšä¾¿ä¸¾ä¾‹ï¼‰ï¼š

```bash
curl "http://127.0.0.1:8000/device/1/metrics/vibration/ai_analysis?threshold=8.0"
```

ä¼šå¾—åˆ°ç±»ä¼¼ï¼š

```json
{
  "device_id": 1,
  "metric_key": "vibration",
  "last_value": 4.5,
  "status": "insufficient",
  "rul_days": null,
  "curve": [
    {"date": "2025-11-20", "value": 4.5},
    ...
  ]
}
```

å½•å…¥æ¬¡æ•°å¤šäº†ä¹‹åï¼Œ`status` ä¼šå˜æˆ `degrading/stable/uncertain/...`ï¼Œ`rul_days` ä¼šç»™å‡ºå¤§è‡´å¤©æ•°ã€‚

---

ä¸‹ä¸€æ­¥å°±å¯ä»¥åœ¨è¿™ä¸ªåç«¯åŸºç¡€ä¸Šï¼Œç”¨ React + ECharts å†™ä¸€ä¸ªç®€å•å‰ç«¯é¡µé¢ï¼šå·¦è¾¹è¡¨å•å½•å…¥ç‚¹æ£€ï¼Œå³è¾¹æ›²çº¿ + RUL ç»“æœï¼Œå®ç°ä¸€ä¸ªå®Œæ•´çš„ EMS + AI ç¦»çº¿åˆ†æ demoã€‚
